{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89369f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import markdown\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d23eb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store save to faiss_index\n"
     ]
    }
   ],
   "source": [
    "from utils import embed\n",
    "from config import CONFIG\n",
    "\n",
    "# embed and save embeddings\n",
    "vector_store = embed(CONFIG[\"document_folder\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c263432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store loaded\n"
     ]
    }
   ],
   "source": [
    "from utils import load\n",
    "\n",
    "vector_store = load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f82f95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import MarkdownTextSplitter\n",
    "import markdown\n",
    "\n",
    "file = r\"C:\\Users\\shewe\\OneDrive\\Documents\\Obsidian Vault\\Vector Database.md\"\n",
    "f = open(file, 'r')\n",
    "markdown_text = f.read()\n",
    "htmlmarkdown=markdown.markdown( f.read() )\n",
    "\n",
    "splitter = MarkdownTextSplitter(chunk_size=200)\n",
    "chunks = splitter.split_text(htmlmarkdown)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "643b678c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Header 1': 'Vector Database'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on)\n",
    "md_header_splits = markdown_splitter.split_text(markdown_text)\n",
    "md_header_splits[0].metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5218a0e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#', 'Header 1'), ('##', 'Header 2'), ('###', 'Header 3')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7653101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1:\n",
      "page_content='# Vector Database  \n",
      "#tag: #concept\n",
      "\n",
      "Also known as **vector DB**, this refers to a specialized database designed to store and search high-dimensional vectors — usually produced by an embedding model.\n",
      "\n",
      "## How It Works\n",
      "\n",
      "1. **Input Data**  \n",
      "   You start with raw data such as text (essays, code, etc.).\n",
      "\n",
      "2. **Embedding**  \n",
      "   An **embedding model** (e.g., OpenAI, Cohere, or sentence-transformers) converts the input into a high-dimensional vector (e.g., 384 or 768 dimensions).'\n",
      "\n",
      "Chunk 2:\n",
      "page_content='3. **Storage**  \n",
      "   These vectors are stored in a vector database like **FAISS**, **Pinecone**, or **Weaviate**.\n",
      "\n",
      "4. **Querying**  \n",
      "   When a user submits a query (e.g., a question), the **same embedding model** is used to convert the query into a vector.\n",
      "\n",
      "5. **Similarity Search**  \n",
      "   The vector DB uses a similarity algorithm (like **cosine similarity** or **approximate nearest neighbors**) to find the most similar vectors to the query.\n",
      "\n",
      "## Metadata Storage and Filtering'\n",
      "\n",
      "Chunk 3:\n",
      "page_content='## Metadata Storage and Filtering\n",
      "\n",
      "Many vector databases allow attaching **metadata** (e.g., tags, source, timestamp) to each vector. This enables:\n",
      "- **Filtering** results (e.g., only search documents tagged `#finance`)\n",
      "- **Ranking** based on custom conditions\n",
      "\n",
      "## Example Use Cases\n",
      "\n",
      "- **RAG (Retrieval-Augmented Generation)**\n",
      "- Semantic search\n",
      "- Recommendation systems\n",
      "\n",
      "## Sources\n",
      "- [Pinecone — What is a Vector Database?](https://www.pinecone.io/learn/vector-database/)'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## recursive text splitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=100,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"#\", \"##\", \"###\"]\n",
    ")\n",
    "\n",
    "doc = Document(page_content=markdown_text)\n",
    "\n",
    "# Split it\n",
    "chunks = text_splitter.split_documents([doc, doc])\n",
    "\n",
    "# See the result\n",
    "for i, chunk in enumerate(chunks[:3]):\n",
    "    print(f\"Chunk {i+1}:\\n{chunk}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "890646d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='# Vector Database  \\n#tag: #concept\\n\\nAlso known as **vector DB**, this refers to a specialized database designed to store and search high-dimensional vectors — usually produced by an embedding model.\\n\\n## How It Works\\n\\n1. **Input Data**  \\n   You start with raw data such as text (essays, code, etc.).\\n\\n2. **Embedding**  \\n   An **embedding model** (e.g., OpenAI, Cohere, or sentence-transformers) converts the input into a high-dimensional vector (e.g., 384 or 768 dimensions).'),\n",
       " Document(metadata={}, page_content='3. **Storage**  \\n   These vectors are stored in a vector database like **FAISS**, **Pinecone**, or **Weaviate**.\\n\\n4. **Querying**  \\n   When a user submits a query (e.g., a question), the **same embedding model** is used to convert the query into a vector.\\n\\n5. **Similarity Search**  \\n   The vector DB uses a similarity algorithm (like **cosine similarity** or **approximate nearest neighbors**) to find the most similar vectors to the query.\\n\\n## Metadata Storage and Filtering'),\n",
       " Document(metadata={}, page_content='## Metadata Storage and Filtering\\n\\nMany vector databases allow attaching **metadata** (e.g., tags, source, timestamp) to each vector. This enables:\\n- **Filtering** results (e.g., only search documents tagged `#finance`)\\n- **Ranking** based on custom conditions\\n\\n## Example Use Cases\\n\\n- **RAG (Retrieval-Augmented Generation)**\\n- Semantic search\\n- Recommendation systems\\n\\n## Sources\\n- [Pinecone — What is a Vector Database?](https://www.pinecone.io/learn/vector-database/)'),\n",
       " Document(metadata={}, page_content='# Vector Database  \\n#tag: #concept\\n\\nAlso known as **vector DB**, this refers to a specialized database designed to store and search high-dimensional vectors — usually produced by an embedding model.\\n\\n## How It Works\\n\\n1. **Input Data**  \\n   You start with raw data such as text (essays, code, etc.).\\n\\n2. **Embedding**  \\n   An **embedding model** (e.g., OpenAI, Cohere, or sentence-transformers) converts the input into a high-dimensional vector (e.g., 384 or 768 dimensions).'),\n",
       " Document(metadata={}, page_content='3. **Storage**  \\n   These vectors are stored in a vector database like **FAISS**, **Pinecone**, or **Weaviate**.\\n\\n4. **Querying**  \\n   When a user submits a query (e.g., a question), the **same embedding model** is used to convert the query into a vector.\\n\\n5. **Similarity Search**  \\n   The vector DB uses a similarity algorithm (like **cosine similarity** or **approximate nearest neighbors**) to find the most similar vectors to the query.\\n\\n## Metadata Storage and Filtering'),\n",
       " Document(metadata={}, page_content='## Metadata Storage and Filtering\\n\\nMany vector databases allow attaching **metadata** (e.g., tags, source, timestamp) to each vector. This enables:\\n- **Filtering** results (e.g., only search documents tagged `#finance`)\\n- **Ranking** based on custom conditions\\n\\n## Example Use Cases\\n\\n- **RAG (Retrieval-Augmented Generation)**\\n- Semantic search\\n- Recommendation systems\\n\\n## Sources\\n- [Pinecone — What is a Vector Database?](https://www.pinecone.io/learn/vector-database/)')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ba8290",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shewe\\AppData\\Local\\Temp\\ipykernel_17780\\3181714049.py:9: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model=\"llama3.2\")\n",
      "C:\\Users\\shewe\\AppData\\Local\\Temp\\ipykernel_17780\\3181714049.py:10: LangChainDeprecationWarning: The method `BaseLLM.predict` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = llm.predict(prompt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode Seeking Loss is a loss function introduced in DFMGAN, which aims to maximize the generation of defects with different latent spaces $z_{defect}$. It is considered an intrinsic motivation technique, meaning it encourages the generator to produce better results without relying on extrinsic rewards or penalties.\n"
     ]
    }
   ],
   "source": [
    "# 3. Search for similar items\n",
    "query = \"what is mode seeking loss\"\n",
    "query_vector = vector_store.similarity_search(query, k=2)\n",
    "\n",
    "context = \"\\n\\n\".join([doc.page_content for doc in query_vector])\n",
    "prompt = f\"Answer the question based on the context below.\\n\\nContext:\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "\n",
    "from langchain_community.llms import Ollama \n",
    "llm = Ollama(model=\"llama3.2\")\n",
    "response = llm.predict(prompt)\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f4428e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "Metadata: {'Title': 'html'}\n",
      "--------------------------------------------------\n",
      "Document 2:\n",
      "Metadata: {'Title': 'html'}\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "query = \"gan\"\n",
    "query_vector = vector_store.similarity_search(query, k=2)\n",
    "\n",
    "for i, doc in enumerate(query_vector):\n",
    "    print(f\"Document {i+1}:\")\n",
    "    print(f\"Metadata: {doc.metadata}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81d47969",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shewe\\AppData\\Local\\Temp\\ipykernel_16168\\3622681122.py:1: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  results = vector_store.as_retriever().get_relevant_documents(\"gan\")\n"
     ]
    }
   ],
   "source": [
    "results = vector_store.as_retriever().get_relevant_documents(\"gan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6db6ffa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "html\n",
      "html\n",
      "html\n",
      "html\n"
     ]
    }
   ],
   "source": [
    "for doc in results:\n",
    "    print(doc.metadata['Title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddf6eea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SelfNoteRAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
