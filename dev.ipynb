{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89369f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import markdown\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d23eb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store save to faiss_index\n"
     ]
    }
   ],
   "source": [
    "from utils import embed\n",
    "from config import CONFIG\n",
    "\n",
    "# embed and save embeddings\n",
    "vector_store = embed(CONFIG[\"document_folder\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c263432",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load\n",
    "\n",
    "vector_store = load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f82f95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import MarkdownTextSplitter\n",
    "import markdown\n",
    "\n",
    "file = r\"C:\\Users\\shewe\\OneDrive\\Documents\\Obsidian Vault\\Vector Database.md\"\n",
    "f = open(file, 'r')\n",
    "markdown_text = f.read()\n",
    "htmlmarkdown=markdown.markdown( f.read() )\n",
    "\n",
    "splitter = MarkdownTextSplitter(chunk_size=200)\n",
    "chunks = splitter.split_text(htmlmarkdown)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "643b678c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Header 1': 'Vector Database'}, page_content='#tag: #concept  \\nAlso known as **vector DB**, this refers to a specialized database designed to store and search high-dimensional vectors — usually produced by an embedding model.'),\n",
       " Document(metadata={'Header 1': 'Vector Database', 'Header 2': 'How It Works'}, page_content='1. **Input Data**\\nYou start with raw data such as text (essays, code, etc.).  \\n2. **Embedding**\\nAn **embedding model** (e.g., OpenAI, Cohere, or sentence-transformers) converts the input into a high-dimensional vector (e.g., 384 or 768 dimensions).  \\n3. **Storage**\\nThese vectors are stored in a vector database like **FAISS**, **Pinecone**, or **Weaviate**.  \\n4. **Querying**\\nWhen a user submits a query (e.g., a question), the **same embedding model** is used to convert the query into a vector.  \\n5. **Similarity Search**\\nThe vector DB uses a similarity algorithm (like **cosine similarity** or **approximate nearest neighbors**) to find the most similar vectors to the query.'),\n",
       " Document(metadata={'Header 1': 'Vector Database', 'Header 2': 'Metadata Storage and Filtering'}, page_content='Many vector databases allow attaching **metadata** (e.g., tags, source, timestamp) to each vector. This enables:\\n- **Filtering** results (e.g., only search documents tagged `#finance`)\\n- **Ranking** based on custom conditions'),\n",
       " Document(metadata={'Header 1': 'Vector Database', 'Header 2': 'Example Use Cases'}, page_content='- **RAG (Retrieval-Augmented Generation)**\\n- Semantic search\\n- Recommendation systems'),\n",
       " Document(metadata={'Header 1': 'Vector Database', 'Header 2': 'Sources'}, page_content='- [Pinecone — What is a Vector Database?](https://www.pinecone.io/learn/vector-database/)')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on)\n",
    "md_header_splits = markdown_splitter.split_text(markdown_text)\n",
    "md_header_splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fad823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>Vector Database</h1>\n",
      "<h1>tag: #concept</h1>\n",
      "<p>Also known as <strong>vector DB</strong>, this refers to a specialized database designed to store and search high-dimensional vectors — usually produced by an embedding model.</p>\n",
      "<h2>How It Works</h2>\n",
      "<ol>\n",
      "<li>\n",
      "<p><strong>Input Data</strong><br />\n",
      "   You start with raw data such as text (essays, code, etc.).</p>\n",
      "</li>\n",
      "<li>\n",
      "<p><strong>Embedding</strong><br />\n",
      "   An <strong>embedding model</strong> (e.g., OpenAI, Cohere, or sentence-transformers) converts the input into a high-dimensional vector (e.g., 384 or 768 dimensions).</p>\n",
      "</li>\n",
      "<li>\n",
      "<p><strong>Storage</strong><br />\n",
      "   These vectors are stored in a vector database like <strong>FAISS</strong>, <strong>Pinecone</strong>, or <strong>Weaviate</strong>.</p>\n",
      "</li>\n",
      "<li>\n",
      "<p><strong>Querying</strong><br />\n",
      "   When a user submits a query (e.g., a question), the <strong>same embedding model</strong> is used to convert the query into a vector.</p>\n",
      "</li>\n",
      "<li>\n",
      "<p><strong>Similarity Search</strong><br />\n",
      "   The vector DB uses a similarity algorithm (like <strong>cosine similarity</strong> or <strong>approximate nearest neighbors</strong>) to find the most similar vectors to the query.</p>\n",
      "</li>\n",
      "</ol>\n",
      "<h2>Metadata Storage and Filtering</h2>\n",
      "<p>Many vector databases allow attaching <strong>metadata</strong> (e.g., tags, source, timestamp) to each vector. This enables:\n",
      "- <strong>Filtering</strong> results (e.g., only search documents tagged <code>#finance</code>)\n",
      "- <strong>Ranking</strong> based on custom conditions</p>\n",
      "<h2>Example Use Cases</h2>\n",
      "<ul>\n",
      "<li><strong>RAG (Retrieval-Augmented Generation)</strong></li>\n",
      "<li>Semantic search</li>\n",
      "<li>Recommendation systems</li>\n",
      "</ul>\n",
      "<h2>Sources</h2>\n",
      "<ul>\n",
      "<li><a href=\"https://www.pinecone.io/learn/vector-database/\">Pinecone — What is a Vector Database?</a></li>\n",
      "</ul>\n"
     ]
    }
   ],
   "source": [
    "print(htmlmarkdown.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1ba8290",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shewe\\AppData\\Local\\Temp\\ipykernel_17780\\3181714049.py:9: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model=\"llama3.2\")\n",
      "C:\\Users\\shewe\\AppData\\Local\\Temp\\ipykernel_17780\\3181714049.py:10: LangChainDeprecationWarning: The method `BaseLLM.predict` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = llm.predict(prompt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode Seeking Loss is a loss function introduced in DFMGAN, which aims to maximize the generation of defects with different latent spaces $z_{defect}$. It is considered an intrinsic motivation technique, meaning it encourages the generator to produce better results without relying on extrinsic rewards or penalties.\n"
     ]
    }
   ],
   "source": [
    "# 3. Search for similar items\n",
    "query = \"what is mode seeking loss\"\n",
    "query_vector = vector_store.similarity_search(query, k=2)\n",
    "\n",
    "context = \"\\n\\n\".join([doc.page_content for doc in query_vector])\n",
    "prompt = f\"Answer the question based on the context below.\\n\\nContext:\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "\n",
    "from langchain_community.llms import Ollama \n",
    "llm = Ollama(model=\"llama3.2\")\n",
    "response = llm.predict(prompt)\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4428e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SelfNoteRAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
